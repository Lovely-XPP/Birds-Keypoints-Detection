# Birds-Keypoints-Detection

├── README.md
├── backup
│   ├── colormap.py
│   ├── demo.py
│   ├── labelme2coco.py
│   ├── train.py
│   └── visualize_coco.py
├── coco
│   ├── annotations
│   ├── train2019
│   └── val2019
├── data_enhance
│   ├── coco2csv.py
│   ├── config.py
│   ├── main.py
│   ├── csv2coco.py
│   ├── dataset_enhance.py
│   └── labelme2coco.py
├── demo.py
├── fig.py
├── input
│   ├── img
│   └── video
├── labelme2coco.py
├── output
│   ├── data
│   └── model
├── output_data.py
├── predictor.py
├── read_data.py
└── train.py

## Introduction

The repo is a bird-keypoints-detection based on Detectron 2. I use `labelme` as the tool to annotate pictures, which generates `json` files. Then, translate the `json` files to `coco` dataset by `labelme2coco.py`. Therefore, we can register the dataset to Detectron 2 and train the model.


## Requirement

### **Install Detectron 2**

Follow the official Tutorials : https://detectron2.readthedocs.io/en/latest/tutorials/install.html

### **Other Modules**

````sh
pip3 install labelme cv2 tqdm argparse
````

## Code Files Description

At this part, I will introduce the function of main code files. For how to use the code, you can read the comment on the head of these code files.

### labelme2coco.py

Translate the labelme annotation format to the coco format.

###  train.py

Train models.

### demo.py

Demonstrate the result of input (pics or videos).

### output_data.py

Output the infomation of the detection results, including boxes bounder, scores, classes, keypoints (if existed).

### fig.py

Visualize the Loss - Iter curve by reading the log file generated by Detectron 2. Here is an example:

### data_enhance/main.py

Enhance annotation datas, such as scaling, adding noise and so on.

## Credits

* [Detectron 2](https://github.com/facebookresearch/detectron2) from [Facebookresearch](https://github.com/facebookresearch).  
* Wah C., Branson S., Welinder P., Perona P., Belongie S. “The Caltech-UCSD Birds-200-2011 Dataset.” Computation & Neural Systems Technical Report, CNS-TR-2011-001.